{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on training a HTS-AT model for audio classification on the ESC-50 Dataset\n",
    "\n",
    "Referece: \n",
    "\n",
    "[HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection, ICASSP 2022](https://arxiv.org/abs/2202.00874)\n",
    "\n",
    "Following the HTS-AT's paper, in this tutorial, we would show how to use the HST-AT in the training of the ESC-50 Dataset.\n",
    "\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories\n",
    "\n",
    "Before running this tutorial, please make sure that you install the below packages by following steps:\n",
    "\n",
    "1. download [the codebase](https://github.com/RetroCirce/HTS-Audio-Transformer), and put this tutorial notebook inside the codebase folder.\n",
    "\n",
    "2. In the github code folder:\n",
    "\n",
    "    > pip install -r requirements.txt\n",
    "\n",
    "3. We do not include the installation of PyTorch in the requirment, since different machines require different vereions of CUDA and Toolkits. So make sure you install the PyTorch from [the official guidance](https://pytorch.org/).\n",
    "\n",
    "4. Install the 'SOX' and the 'ffmpeg', we recommend that you run this code in Linux inside the Conda environment. In that, you can install them by:\n",
    "\n",
    "    > sudo apt install sox\n",
    "    \n",
    "    > conda install -c conda-forge ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime \n",
    "import wget\n",
    "import sys\n",
    "import gdown\n",
    "import zipfile\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "import json\n",
    "from collections import Counter\n",
    "# in the notebook, we only can use one GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workspace and download the needed files\n",
    "\n",
    "# def create_path(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.mkdir(path)\n",
    "\n",
    "# workspace = \"./workspace\"\n",
    "# dataset_path = os.path.join(workspace, \"esc-50\")\n",
    "# checkpoint_path = os.path.join(workspace, \"ckpt\")\n",
    "# esc_raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "\n",
    "# create_path(workspace)\n",
    "# create_path(dataset_path)\n",
    "# create_path(checkpoint_path)\n",
    "# create_path(esc_raw_path)\n",
    "\n",
    "\n",
    "# # download the esc-50 dataset\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_path, 'ESC-50-master.zip')):\n",
    "#     print(\"-------------Downloading ESC-50 Dataset-------------\")\n",
    "#     wget.download('https://github.com/karoldvl/ESC-50/archive/master.zip', out=dataset_path)\n",
    "#     with zipfile.ZipFile(os.path.join(dataset_path, 'ESC-50-master.zip'), 'r') as zip_ref:\n",
    "#         zip_ref.extractall(esc_raw_path)\n",
    "#     print(\"-------------Success-------------\")\n",
    "\n",
    "# if not os.path.exists(os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt')):\n",
    "#     gdown.download(id='1OK8a5XuMVLyeVKF117L8pfxeZYdfSDZv', output=os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ESC-50 Dataset\n",
    "# meta_path = os.path.join(esc_raw_path, 'ESC-50-master', 'meta', 'esc50.csv')\n",
    "# audio_path = os.path.join(esc_raw_path, 'ESC-50-master', 'audio')\n",
    "# resample_path = os.path.join(dataset_path, 'resample')\n",
    "# savedata_path = os.path.join(dataset_path, 'esc-50-data.npy')\n",
    "# create_path(resample_path)\n",
    "\n",
    "# meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "# audio_list = os.listdir(audio_path)\n",
    "\n",
    "# # resample\n",
    "# print(\"-------------Resample ESC-50-------------\")\n",
    "# for f in audio_list:\n",
    "#     full_f = os.path.join(audio_path, f)\n",
    "#     resample_f = os.path.join(resample_path, f)\n",
    "#     if not os.path.exists(resample_f):\n",
    "#         os.system('sox -V1 ' + full_f + ' -r 32000 ' + resample_f)\n",
    "# print(\"-------------Success-------------\")\n",
    "\n",
    "# print(\"-------------Build Dataset-------------\")\n",
    "# output_dict = [[] for _ in range(5)]\n",
    "# for label in meta:\n",
    "#     name = label[0]\n",
    "#     fold = label[1]\n",
    "#     target = label[2]\n",
    "#     y, sr = librosa.load(os.path.join(resample_path, name), sr = None)\n",
    "#     output_dict[int(fold) - 1].append(\n",
    "#         {\n",
    "#             \"name\": name,\n",
    "#             \"target\": int(target),\n",
    "#             \"waveform\": y\n",
    "#         }\n",
    "#     )\n",
    "# np.save(savedata_path, output_dict)\n",
    "# print(\"-------------Success-------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "\n",
    "from utils import create_folder, dump_config, process_idc\n",
    "import config \n",
    "from sed_model import SEDWrapper, Ensemble_SEDWrapper\n",
    "from data_generator import ESC_Dataset\n",
    "from model.htsat import HTSAT_Swin_Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, eval_dataset, device_num):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.device_num = device_num\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        train_loader = DataLoader(\n",
    "            dataset = self.train_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = train_sampler\n",
    "        )\n",
    "        return train_loader\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        eval_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = eval_sampler\n",
    "        )\n",
    "        return eval_loader\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        test_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = test_sampler\n",
    "        )\n",
    "        return test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the workspace\n",
    "# device_num = torch.cuda.device_count()\n",
    "# print(\"each batch size:\", config.batch_size // device_num)\n",
    "\n",
    "# full_dataset = np.load(os.path.join(config.dataset_path, \"esc-50-data.npy\"), allow_pickle = True)\n",
    "\n",
    "# # set exp folder\n",
    "# exp_dir = os.path.join(config.workspace, \"results\", config.exp_name)\n",
    "# checkpoint_dir = os.path.join(config.workspace, \"results\", config.exp_name, \"checkpoint\")\n",
    "# if not config.debug:\n",
    "#     create_folder(os.path.join(config.workspace, \"results\"))\n",
    "#     create_folder(exp_dir)\n",
    "#     create_folder(checkpoint_dir)\n",
    "#     dump_config(config, os.path.join(exp_dir, config.exp_name), False)\n",
    "\n",
    "# print(\"Using ESC\")\n",
    "# dataset = ESC_Dataset(\n",
    "#     dataset = full_dataset,\n",
    "#     config = config,\n",
    "#     eval_mode = False\n",
    "# )\n",
    "# eval_dataset = ESC_Dataset(\n",
    "#     dataset = full_dataset,\n",
    "#     config = config,\n",
    "#     eval_mode = True\n",
    "# )\n",
    "\n",
    "# audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor = \"acc\",\n",
    "#     filename='l-{epoch:d}-{acc:.3f}',\n",
    "#     save_top_k = 20,\n",
    "#     mode = \"max\"\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Trainer\n",
    "# trainer = pl.Trainer(\n",
    "#     deterministic=True,\n",
    "#     default_root_dir = checkpoint_dir,\n",
    "#     gpus = device_num, \n",
    "#     val_check_interval = 1.0,\n",
    "#     max_epochs = config.max_epoch,\n",
    "#     auto_lr_find = True,    \n",
    "#     sync_batchnorm = True,\n",
    "#     callbacks = [checkpoint_callback],\n",
    "#     accelerator = \"ddp\" if device_num > 1 else None,\n",
    "#     num_sanity_val_steps = 0,\n",
    "#     resume_from_checkpoint = None, \n",
    "#     replace_sampler_ddp = False,\n",
    "#     gradient_clip_val=1.0\n",
    "# )\n",
    "\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config = config,\n",
    "    depths = config.htsat_depth,\n",
    "    embed_dim = config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "model = SEDWrapper(\n",
    "    sed_model = sed_model, \n",
    "    config = config,\n",
    "    dataset = ''\n",
    ")\n",
    "\n",
    "if config.resume_checkpoint is not None:\n",
    "    print(\"Load Checkpoint from \", config.resume_checkpoint)\n",
    "    ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.weight\")\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.bias\")\n",
    "    # finetune on the esc and spv2 dataset\n",
    "    # ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.weight\")\n",
    "    # ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.bias\")\n",
    "    # model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the model\n",
    "# # You can set different fold index by setting 'esc_fold' to any number from 0-4 in esc_config.py\n",
    "# trainer.fit(model, audioset_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let us Check the Result\n",
    "\n",
    "Find the path of your saved checkpoint and paste it in the below variable.\n",
    "Then you are able to follow the below code for checking the prediction result of any sample you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer the single data to check the result\n",
    "# get a model you saved\n",
    "model_path = ''\n",
    "\n",
    "# get the groundtruth\n",
    "# meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "# gd = {}\n",
    "# for label in meta:\n",
    "#     name = label[0]\n",
    "#     target = label[2]\n",
    "#     gd[name] = target\n",
    "\n",
    "class Audio_Classification:\n",
    "    def __init__(self, model_path, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.sed_model = HTSAT_Swin_Transformer(\n",
    "            spec_size=config.htsat_spec_size,\n",
    "            patch_size=config.htsat_patch_size,\n",
    "            in_chans=1,\n",
    "            num_classes=config.classes_num,\n",
    "            window_size=config.htsat_window_size,\n",
    "            config = config,\n",
    "            depths = config.htsat_depth,\n",
    "            embed_dim = config.htsat_dim,\n",
    "            patch_stride=config.htsat_stride,\n",
    "            num_heads=config.htsat_num_head\n",
    "        )\n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "        temp_ckpt = {}\n",
    "        for key in ckpt[\"state_dict\"]:\n",
    "            temp_ckpt[key[10:]] = ckpt['state_dict'][key]\n",
    "        self.sed_model.load_state_dict(temp_ckpt)\n",
    "        self.sed_model.to(self.device)\n",
    "        self.sed_model.eval()\n",
    "\n",
    "\n",
    "    def predict(self, audiofile):\n",
    "\n",
    "        if audiofile:\n",
    "            waveform, sr = librosa.load(audiofile, sr=32000)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(waveform).float().to(self.device)\n",
    "                output_dict = self.sed_model(x[None, :], None, True)\n",
    "                # print(output_dict)\n",
    "                pred = output_dict['clipwise_output']\n",
    "                pred_post = pred[0].detach().cpu().numpy()\n",
    "                pred_label = np.argmax(pred_post)\n",
    "                pred_prob = np.max(pred_post)\n",
    "                if pred_label == 0:\n",
    "                    pred_post = np.delete(pred_post, 0)\n",
    "                    pred_label = np.argmax(pred_post)\n",
    "                    pred_prob = np.max(pred_post)\n",
    "                    pred_label = np.argmax(np.delete(pred_post, 0))+1\n",
    "                    pred_prob = np.max(np.delete(pred_post, 0))\n",
    "                if pred_label == 431:\n",
    "                    pred_post = np.delete(pred_post, 431)\n",
    "                    pred_label = np.argmax(pred_post)\n",
    "                    pred_prob = np.max(pred_post)\n",
    "                    pred_label = np.argmax(np.delete(pred_post, 431))+1\n",
    "                    pred_prob = np.max(np.delete(pred_post, 431))\n",
    "                    \n",
    "            return pred_label, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gunshot, gunfire', list([426]), list([428, 429, 430, 431])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[427]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myanmar\n"
     ]
    }
   ],
   "source": [
    "classes = [\"Emergency vehicle\", \"Gunshot, gunfire\", \"Helicopter\", \"Fireworks\", \"Explosion\", \n",
    "           \"Civil defense siren\", \"Cough\", \"Motorcycle\", \"Race car, auto racing\", \n",
    "           \"Crying, sobbing\", \"Fire alarm\", 'Vehicle', \"Car\", \"Motor vehicle (road)\",\n",
    "           \"Skidding\", \"Battle cry\", \"Accelerating, revving, vroom\", \"Police car (siren)\",\n",
    "           \"Fire engine, fire truck (siren)\", \"Conversation\", \"Artillery fire\", \"Alarm\",\n",
    "           \"Machine gun\", \"Crowd\", \"Ambulance (siren)\", \"Firecracker\", \"Whip\", \"Toot\" \"Snort\",\n",
    "           \"Cap gun\", \"Fusillade\", \"Yell\"]\n",
    "\n",
    "n=3\n",
    "my_clip = mp.VideoFileClip('video/myanmar.mp4')\n",
    "remainder = int(my_clip.duration % n)\n",
    "int_seg = int(my_clip.duration - remainder)\n",
    "durations = list(range(0, int_seg+n, n))\n",
    "paired_durations = [[x, y] for x, y in zip(durations, durations[1:])]\n",
    "file_name = my_clip.filename.split('/')[1].split('.')[0]\n",
    "# file_name = file_name.replace('_', '')\n",
    "print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'video/{file_name}', exist_ok=True)\n",
    "\n",
    "# creates audio clips\n",
    "for i in paired_durations:\n",
    "    clips = my_clip.audio.subclip(t_start=i[0], t_end=i[1])\n",
    "    clips.write_audiofile(f'video/{file_name}/{file_name}_{i[0]}_{i[1]}.wav', logger=None)\n",
    "\n",
    "# clips remainder of video\n",
    "if remainder > 0:\n",
    "    remainder_clip = my_clip.audio.subclip(t_start=paired_durations[-1][1], t_end=paired_durations[-1][1]+remainder)\n",
    "    remainder_clip.write_audiofile(f'video/{file_name}/{file_name}_{paired_durations[-1][1]}_{paired_durations[-1][1]+remainder}.wav', logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_integer(filename):\n",
    "    return int(filename.split('.')[0].split('_')[1])\n",
    "\n",
    "audio_list = sorted(os.listdir(f'video/{file_name}'), key=extract_integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_classes = {\n",
    "'Gunshot/Boom': {\"Gunshot, gunfire\", \"Fireworks\", \"Explosion\", \"Artillery fire\", \"Machine gun\", \"Firecracker\", \"Whip\", \"Cap gun\", \"Fusillade\"},\n",
    "'Siren/Alarm': {\"Fire alarm\", \"Emergency vehicle\", \"Civil defense siren\", \"Police car (siren)\", \"Fire engine, fire truck (siren)\", \"Alarm\", \"Ambulance (siren)\"},\n",
    "'Coughing/Gasping': {\"Snort\", \"Cough\"},\n",
    "'Crying': {\"Crying, sobbing\"},\n",
    "'Helicopter': {\"Helicopter\"},\n",
    "'Conversation': {\"Conversation\"},\n",
    "'Crowd Noise': {\"Crowd\"},\n",
    "'Yelling': {\"Battle cry\", \"Yell\"},\n",
    "'Horn': {\"Toot\"},\n",
    "'Vehicle Skidding': {\"Skidding\"},\n",
    "'Vehicle Noise': {\"Motorcycle\", \"Race car, auto racing\", \"Vehicle\", \"Car\", \"Motor vehicle (road)\", \"Accelerating, revving, vroom\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKey(val):\n",
    "    for key, value in filtered_classes.items():\n",
    "        if val in value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs audio classification on clips in directory\n",
    "results_dict = {}\n",
    "unfiltered = []\n",
    "# performs audio classification on clips in directory\n",
    "Audiocls = Audio_Classification(model_path, config)\n",
    "for clip in audio_list:\n",
    "    pred_label, pred_prob = Audiocls.predict(f'video/{file_name}/{clip}')\n",
    "    unfiltered.append((clip, data[pred_label][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "# converts class detections to per second \n",
    "for entry in unfiltered:\n",
    "    slice_ts = entry[0].split('.')[0].split('_')[1:]\n",
    "    for sec in range(int(slice_ts[0]), int(slice_ts[-1])):\n",
    "        filtered.append((sec+1, entry[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters to only classes in filtered_classes\n",
    "audioData = [(i[0], GetKey(i[1])) for i in filtered if i[1] in classes]\n",
    "# Counts # of detections per class \n",
    "totals = dict(Counter([i[1] for i in audioData]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles and writes to dictionary\n",
    "results_dict['uniqueId'] = my_clip.filename.split('video/')[1]\n",
    "results_dict['totals'] = totals\n",
    "results_dict['audioData'] = audioData\n",
    "results_dict['audioGraph'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uniqueId': 'myanmar.mp4',\n",
       " 'totals': {'Gunshot/Boom': 12, 'Crowd Noise': 3},\n",
       " 'audioData': [(64, 'Gunshot/Boom'),\n",
       "  (65, 'Gunshot/Boom'),\n",
       "  (66, 'Gunshot/Boom'),\n",
       "  (70, 'Gunshot/Boom'),\n",
       "  (71, 'Gunshot/Boom'),\n",
       "  (72, 'Gunshot/Boom'),\n",
       "  (76, 'Gunshot/Boom'),\n",
       "  (77, 'Gunshot/Boom'),\n",
       "  (78, 'Gunshot/Boom'),\n",
       "  (88, 'Gunshot/Boom'),\n",
       "  (89, 'Gunshot/Boom'),\n",
       "  (90, 'Gunshot/Boom'),\n",
       "  (115, 'Crowd Noise'),\n",
       "  (116, 'Crowd Noise'),\n",
       "  (117, 'Crowd Noise')],\n",
       " 'audioGraph': []}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hts')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20ba89a13c7891bbac7592fe1da5d7897681459127eb55a506e50618a321f447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
